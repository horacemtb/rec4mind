{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e48mI8dAKb0y"
   },
   "source": [
    "In this notebook, we will train a custom neural network to generate item text embeddings using a contrastive learning framework. This approach will enable us to obtain custom item text embeddings of a specified dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from google.colab import drive\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import random\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM = 64\n",
    "BATCH_SIZE = 1024 #256\n",
    "EPOCHS = 300\n",
    "TEMPERATURE = 0.07  # NT-Xent loss temperature\n",
    "LR = 1e-3 #1e-4\n",
    "MODEL_NAME = \"distilbert-base-uncased\"  # Transformer model for encoding text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = '/content/drive/MyDrive/ML/Reinforcement Learning/Final project/MIND'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train_path = os.path.join(project_dir, 'MINDsmall_train/news.tsv')\n",
    "news_train = pd.read_csv(news_train_path, sep='\\t', header=None, names=[\"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"title_entities\", \"abstract_entities\"])\n",
    "\n",
    "news_dev_path = os.path.join(project_dir, 'MINDsmall_dev/news.tsv')\n",
    "news_dev = pd.read_csv(news_dev_path, sep='\\t', header=None, names=[\"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"title_entities\", \"abstract_entities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors_train_path = os.path.join(project_dir, 'processed_data/merged_behaviors_train.csv')\n",
    "behaviors_train = pd.read_csv(behaviors_train_path)\n",
    "\n",
    "behaviors_dev_path = os.path.join(project_dir, 'processed_data/merged_behaviors_dev.csv')\n",
    "behaviors_dev = pd.read_csv(behaviors_dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any users are found in both train and dev sets\n",
    "\n",
    "len(set(behaviors_train['user_id']).intersection(set(behaviors_dev['user_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate train and dev sets into a single dataframe\n",
    "# remove user_id duplicates\n",
    "# filter out users with histories consisting of less than 2 items\n",
    "\n",
    "behaviors_df = pd.concat([behaviors_train, behaviors_dev], ignore_index=True, sort=False)\n",
    "behaviors_df.drop_duplicates(subset='user_id', inplace=True, ignore_index=True)\n",
    "behaviors_df = behaviors_df[behaviors_df['history'].apply(lambda x: len(x.split()))>1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_behaviors(row):\n",
    "    # Get item IDs of clicked impressions (flag == 1)\n",
    "    clicked_impressions = \" \".join(i.split('-')[0] for i in row['impressions'].split() if int(i.split('-')[1]) == 1)\n",
    "\n",
    "    # Get item IDs of non-clicked impressions (flag == 0)\n",
    "    not_clicked_impressions = \" \".join(i.split('-')[0] for i in row['impressions'].split() if int(i.split('-')[1]) == 0)\n",
    "\n",
    "    # Append clicked items to the user's history\n",
    "    row['history'] += \" \" + clicked_impressions\n",
    "\n",
    "    # Update impressions to only include non-clicked items\n",
    "    row['impressions'] = not_clicked_impressions\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors_df = behaviors_df.apply(process_behaviors, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors_df = behaviors_df[behaviors_df['impressions'].apply(lambda x: len(x.split()))>0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = behaviors_df['history'].apply(lambda x: x.split()).values\n",
    "negative_interactions = behaviors_df['impressions'].apply(lambda x: x.split()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate train and dev news sets into a single dataframe\n",
    "# remove news_id duplicates\n",
    "\n",
    "news_df = pd.concat([news_train, news_dev], ignore_index=True, sort=False)\n",
    "news_df.drop_duplicates(subset='news_id', inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_subcategory(category, subcategory):\n",
    "\n",
    "    if '-' in subcategory or '_' in subcategory:\n",
    "        subcategory = subcategory.replace('-', ' ').replace('_', ' ')\n",
    "        return subcategory\n",
    "\n",
    "    if category.lower() in subcategory.lower():\n",
    "        return ''\n",
    "    else:\n",
    "        return subcategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['proc_subcategory'] = news_df.apply(lambda row: preprocess_subcategory(row['category'], row['subcategory']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df[\"text\"] = news_df[\"category\"] + \" \"\\\n",
    "+ news_df[\"proc_subcategory\"] + \" \"\\\n",
    "+ news_df[\"title\"] + \" \"\\\n",
    "+ news_df[\"abstract\"].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df[\"text\"] = news_df[\"text\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_texts = news_df.set_index('news_id')['text'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model_transformer = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model_transformer.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_transformer.to(device)\n",
    "\n",
    "precomputed_embeddings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article_id, text in tqdm(article_texts.items(), total=len(article_texts)):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        embedding = model_transformer(**inputs).last_hidden_state[:, 0, :].squeeze(0)\n",
    "    precomputed_embeddings[article_id] = embedding.cpu()  # store on CPU for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_path = os.path.join(project_dir, \"precomputed_distilbert_embeddings.pkl\")\n",
    "\n",
    "# with open(embeddings_path, \"wb\") as f:\n",
    "#     pickle.dump(precomputed_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_path = os.path.join(project_dir, \"precomputed_distilbert_embeddings.pkl\")\n",
    "\n",
    "# with open(embeddings_path, \"rb\") as f:\n",
    "#     precomputed_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = precomputed_embeddings[next(iter(precomputed_embeddings))].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3H_zpqeYqR7"
   },
   "source": [
    "A custom dataset class samples triplets—anchor, positive, and negative examples—from user interactions. For each instance, it randomly selects two items from positive interactions (anchor and positive) and one from negative interactions, using precomputed embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveDataset(Dataset):\n",
    "    def __init__(self, interactions, negative_interactions, precomputed_embeddings):\n",
    "        self.precomputed_embeddings = precomputed_embeddings\n",
    "        self.samples = []\n",
    "\n",
    "        for pos_history, neg_history in zip(interactions, negative_interactions):\n",
    "            anchor, positive = random.sample(pos_history, 2)\n",
    "            neg_article = random.choice(neg_history)\n",
    "            self.samples.append((anchor, positive, neg_article))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor, positive, negative = self.samples[idx]\n",
    "        return (\n",
    "            self.precomputed_embeddings[anchor],\n",
    "            self.precomputed_embeddings[positive],\n",
    "            self.precomputed_embeddings[negative]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(interactions))\n",
    "train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "train_pos = interactions[train_idx]\n",
    "val_pos = interactions[val_idx]\n",
    "train_neg = negative_interactions[train_idx]\n",
    "val_neg = negative_interactions[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ContrastiveDataset(train_pos, train_neg, precomputed_embeddings)\n",
    "val_dataset = ContrastiveDataset(val_pos, val_neg, precomputed_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (pos1, pos2, neg) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx + 1}\")\n",
    "    print(\"Positive article 1:\", pos1[0])\n",
    "    print(\"Positive article 2:\", pos2[0])\n",
    "    print(\"Negative article:\", neg[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkFW6NMVaxXW"
   },
   "source": [
    "A neural network (TextEncoder) transforms high-dimensional embeddings (e.g., from distilbert-base-uncased) into a lower-dimensional space. It uses two fully connected layers with ReLU activation and dropout to produce compact item embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, hidden_size, emb_dim=64, dropout_prob=0.5):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc2 = nn.Linear(hidden_size // 2, emb_dim)\n",
    "\n",
    "    def forward(self, embedding_batch):\n",
    "        x = self.fc1(embedding_batch)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJny9AJ1bu51"
   },
   "source": [
    "The NT-Xent loss function computes cosine similarities between the anchor-positive and anchor-negative pairs. It scales these similarities using a temperature parameter and applies binary cross-entropy loss to encourage the model to pull similar items together while pushing dissimilar ones apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTXentLoss(nn.Module):\n",
    "    def __init__(self, temperature=TEMPERATURE):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cosine_similarity = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def forward(self, pos1, pos2, neg):\n",
    "        pos_sim = self.cosine_similarity(pos1, pos2) / self.temperature\n",
    "        neg_sim = self.cosine_similarity(pos1, neg) / self.temperature\n",
    "\n",
    "        logits = torch.cat([pos_sim, neg_sim], dim=0)\n",
    "        labels = torch.cat([torch.ones_like(pos_sim), torch.zeros_like(neg_sim)], dim=0)\n",
    "\n",
    "        return nn.functional.binary_cross_entropy_with_logits(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextEncoder(hidden_size, EMB_DIM)\n",
    "loss_fn = NTXentLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "loss_fn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    train_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Training\", leave=False)\n",
    "\n",
    "    for text1, text2, text_neg in train_progress:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        text1 = text1.to(device)\n",
    "        text2 = text2.to(device)\n",
    "        text_neg = text_neg.to(device)\n",
    "\n",
    "        emb1 = model(text1)\n",
    "        emb2 = model(text2)\n",
    "        emb_neg = model(text_neg)\n",
    "\n",
    "        loss = loss_fn(emb1, emb2, emb_neg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        train_progress.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        val_progress = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Validation\", leave=False)\n",
    "\n",
    "        for text1, text2, text_neg in val_progress:\n",
    "\n",
    "            text1 = text1.to(device)\n",
    "            text2 = text2.to(device)\n",
    "            text_neg = text_neg.to(device)\n",
    "\n",
    "            emb1 = model(text1)\n",
    "            emb2 = model(text2)\n",
    "            emb_neg = model(text_neg)\n",
    "\n",
    "            loss_val = loss_fn(emb1, emb2, emb_neg)\n",
    "            val_loss += loss_val.item()\n",
    "            val_progress.set_postfix(loss=loss_val.item())\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # Check for improvement on validation loss and save the best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        early_stopping_counter = 0\n",
    "        torch.save(model.state_dict(), os.path.join(project_dir, 'text_encoder_distilbest_model.pt'))\n",
    "        print(\"New best model saved!\")\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    scheduler.step(avg_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(1, len(train_losses) + 1))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(epochs, train_losses, marker='o', linestyle='-', color='blue', label='Train Loss')\n",
    "plt.plot(epochs, val_losses, marker='o', linestyle='-', color='red', label='Val Loss')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train and Val Losses Over Epochs')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = TextEncoder(768, 64)\n",
    "best_model.load_state_dict(torch.load(os.path.join(project_dir, 'text_encoder_distilbest_model.pt')))\n",
    "best_model.to(device)\n",
    "best_model.eval()\n",
    "\n",
    "article_embeddings = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for news_id, text in precomputed_embeddings.items():\n",
    "        text_tensor = text.unsqueeze(0).to(device)\n",
    "        embedding = best_model(text_tensor)\n",
    "        article_embeddings[news_id] = embedding.squeeze(0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Min value: {np.min(np.concatenate(list(article_embeddings.values())))}\")\n",
    "print(f\"Max value: {np.max(np.concatenate(list(article_embeddings.values())))}\")\n",
    "\n",
    "# Might need normalization later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_path = os.path.join(project_dir, \"all_news_custom_embeddings.pkl\")\n",
    "\n",
    "# with open(embeddings_path, \"wb\") as f:\n",
    "#     pickle.dump(article_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = os.path.join(project_dir, \"all_news_custom_embeddings.pkl\")\n",
    "\n",
    "with open(embeddings_path, \"rb\") as f:\n",
    "    article_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gaDapqtdxK6"
   },
   "source": [
    "Now split the custom embeddings into train and dev and save them for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = set(news_train[\"news_id\"])\n",
    "dev_ids = set(news_dev[\"news_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_custom_embeddings = {news_id: emb\n",
    "                           for news_id, emb in article_embeddings.items()\n",
    "                           if news_id in train_ids}\n",
    "\n",
    "test_custom_embeddings = {news_id: emb\n",
    "                          for news_id, emb in article_embeddings.items()\n",
    "                          if news_id in dev_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_embeddings_path = os.path.join(project_dir, 'embeddings/news_train_custom_embeddings.pkl')\n",
    "# dev_embeddings_path = os.path.join(project_dir, 'embeddings/news_dev_custom_embeddings.pkl')\n",
    "\n",
    "# with open(train_embeddings_path, \"wb\") as f_train:\n",
    "#     pickle.dump(train_custom_embeddings, f_train)\n",
    "\n",
    "# with open(dev_embeddings_path, \"wb\") as f_test:\n",
    "#     pickle.dump(test_custom_embeddings, f_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
