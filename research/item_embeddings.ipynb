{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7X5ai1-Ttrol"
   },
   "source": [
    "In this notebook, we will generate item embeddings from text data that will later serve as representations of user states and actions in our reward simulator and environment.\n",
    "\n",
    "First, we will examine the text data by analyzing it, performing text preprocessing, and training a custom fastText model.\n",
    "\n",
    "Second, we will explore pretrained Sentence-BERT embeddings.\n",
    "\n",
    "Third, we will leverage combined title and abstract entity embeddings provided as part of the MIND dataset.\n",
    "\n",
    "All embeddings will be saved as lookup tables for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from google.colab import drive\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import classification_report, log_loss, silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from gensim.models import FastText\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = '/content/drive/MyDrive/ML/Reinforcement Learning/Final project/MIND'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors_train_path = os.path.join(project_dir, 'MINDsmall_train/behaviors.tsv')\n",
    "\n",
    "behaviors_train = pd.read_csv(behaviors_train_path, sep='\\t', header=None, names=[\"impression_id\", \"user_id\", \"time\", \"history\", \"impressions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train_path = os.path.join(project_dir, 'MINDsmall_train/news.tsv')\n",
    "\n",
    "news_train = pd.read_csv(news_train_path, sep='\\t', header=None, names=[\"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"title_entities\", \"abstract_entities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_articles = news_train['news_id'].nunique()\n",
    "print(f\"Number of unique articles: {num_articles}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqoZ4n8xNhb3"
   },
   "source": [
    "## Look at the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values in the four columns that would comprise a single text field, from which embeddings would be generated\n",
    "\n",
    "news_train[[\"category\", \"subcategory\", \"title\", \"abstract\"]].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values in the abstract column with empty strings\n",
    "\n",
    "news_train[\"abstract\"] = news_train[\"abstract\"].fillna(\"\")\n",
    "(news_train[[\"abstract\"]] == \"\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "sns.countplot(data=news_train, x=\"category\", hue=\"category\", order=news_train[\"category\"].value_counts().index, palette=\"tab20\")\n",
    "\n",
    "plt.xlabel(\"Category\", fontsize=12)\n",
    "plt.ylabel(\"Count\", fontsize=12)\n",
    "plt.title(\"Distribution of News Categories\", fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_palette = dict(zip(news_train[\"category\"].unique(), sns.color_palette(\"tab20\", n_colors=news_train[\"category\"].nunique())))\n",
    "\n",
    "plt.figure(figsize=(14, 77))\n",
    "sns.countplot(\n",
    "    data=news_train,\n",
    "    y=\"subcategory\",\n",
    "    order=news_train[\"subcategory\"].value_counts().index,\n",
    "    hue=\"category\",\n",
    "    palette=category_palette,\n",
    "    dodge=False,\n",
    "    legend=False\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Count\", fontsize=12)\n",
    "plt.ylabel(\"Subcategory\", fontsize=12)\n",
    "plt.title(\"Distribution of News Subcategories (Colored by Category)\", fontsize=14)\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train['len_title'] = news_train['title'].apply(lambda x: len(x))\n",
    "news_train['len_abstract'] = news_train['abstract'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.histplot(news_train['len_title'], bins=50, kde=True, ax=axes[0])\n",
    "axes[0].set_title(\"Title Length\")\n",
    "axes[0].set_xlabel(\"Num Char\")\n",
    "\n",
    "sns.histplot(news_train[news_train['len_abstract'] > 0]['len_abstract'], bins=50, kde=True, ax=axes[1]) # Filter out samples with no abstracts\n",
    "axes[1].set_title(\"Abstract Length\")\n",
    "axes[1].set_xlabel(\"Num Char\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwNg6ptJ7bQW"
   },
   "source": [
    "## Embedding text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TAx8X-27gI9"
   },
   "source": [
    "Given that our dataset contains many distinct categories and an extensive number of subcategories—both of which are highly imbalanced—we will merge the \"category\", \"subcategory\", \"title\", and \"abstract\" columns into a single text feature. Additionally, we will further process the subcategory column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_subcategory(category, subcategory):\n",
    "\n",
    "    if '-' in subcategory or '_' in subcategory:\n",
    "        subcategory = subcategory.replace('-', ' ').replace('_', ' ')\n",
    "        return subcategory\n",
    "\n",
    "    if category.lower() in subcategory.lower():\n",
    "        return ''\n",
    "    else:\n",
    "        return subcategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train['proc_subcategory'] = news_train.apply(lambda row: preprocess_subcategory(row['category'], row['subcategory']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train[\"text\"] = news_train[\"category\"] + \" \"\\\n",
    "+ news_train[\"proc_subcategory\"] + \" \"\\\n",
    "+ news_train[\"title\"] + \" \"\\\n",
    "+ news_train[\"abstract\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REAmzqFX9MLR"
   },
   "source": [
    "### Approach 1: Train a custom FastText model with embedding size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) | set(ENGLISH_STOP_WORDS) | {\"n't\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", \"'d\", \"'t\", \"wo\", \"ca\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace multiple whitespace characters with a single space.\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Remove standalone numbers from the text.\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "\n",
    "    # Remove punctuation from the text.\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Tokenize the text into individual words.\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words from the tokens.\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Remove duplicate tokens by converting the list to a set, then back to a list.\n",
    "    tokens = list(set(tokens))\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train['clean_text'] = news_train['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at a single example\n",
    "\n",
    "print(f\"Before processing: {news_train['text'].iloc[0]}\")\n",
    "print(f\"After processing: {news_train['clean_text'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train['len_tokens'] = news_train['clean_text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(news_train['len_tokens'], bins=50, kde=True)\n",
    "plt.xlabel(\"Num Tokens\")\n",
    "plt.title(\"Tokens Length after Text Cleaning\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = FastText(vector_size=64, window=3, min_count=3)\n",
    "ft_model.build_vocab(corpus_iterable = news_train['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a closer look at vocab\n",
    "\n",
    "vocab = ft_model.wv\n",
    "vocabulary_words = list(vocab.key_to_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulary_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "ft_model.train(corpus_iterable=news_train['clean_text'], total_examples=len(news_train), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and then load the model if you need it. But we will save the embeddings as look-up tables later on.\n",
    "\n",
    "# ft_model.save(os.path.join(project_dir, 'fasttext_model.bin'))\n",
    "# ft_model = FastText.load(os.path.join(project_dir, 'fasttext_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's perform a couple of sanity checks. We expect the model to correctly compute similarity.\n",
    "\n",
    "ft_model.wv.most_similar(positive=['politics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.wv.most_similar(positive=['football'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.wv.most_similar(positive=['health'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.wv.most_similar(positive=['life'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.wv.most_similar(positive=['royal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.wv.most_similar(positive=['trump'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the mean embedding for an article's tokens using the given model\n",
    "\n",
    "def get_mean_embedding(tokens, model):\n",
    "    return np.mean([model.wv[token] for token in tokens], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train['fasttext_embedding'] = news_train['clean_text'].apply(lambda x: get_mean_embedding(x, ft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXlb0shLsWTC"
   },
   "source": [
    "Let's estimate the quality of the resulting embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_embeddings = np.vstack(news_train['fasttext_embedding'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(init = 'k-means++', n_init=10, random_state=97)\n",
    "visualizer = KElbowVisualizer(model, k = (1, 20))\n",
    "visualizer.fit(fasttext_embeddings)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 5\n",
    "\n",
    "kmeans_model = KMeans(n_init=10, n_clusters=n_clusters, random_state=97)\n",
    "kmeans_model.fit(fasttext_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_sc = silhouette_score(fasttext_embeddings, kmeans_model.labels_)\n",
    "davies_bouldin_sc = davies_bouldin_score(fasttext_embeddings, kmeans_model.labels_)\n",
    "calinski_harabasz_sc = calinski_harabasz_score(fasttext_embeddings, kmeans_model.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Silhouette Score: {silhouette_sc}')\n",
    "print(f'Davies-Bouldin Index: {davies_bouldin_sc}')\n",
    "print(f'Calinski-Harabasz Index: {calinski_harabasz_sc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components = 2, random_state = 88)\n",
    "X_tsne = tsne.fit_transform(fasttext_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsne = pd.DataFrame(X_tsne, columns = ['Dim1', 'Dim2'])\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.scatter(df_tsne['Dim1'], df_tsne['Dim2'], c = 'grey', s = 10, alpha = 0.5)\n",
    "plt.title('t-SNE visualization of fasttext embeddings')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsne = pd.DataFrame(X_tsne, columns = ['Dim1', 'Dim2'])\n",
    "df_tsne['category'] = news_train['category'].values\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df_tsne['category_encoded'] = label_encoder.fit_transform(df_tsne['category'])\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.scatter(df_tsne['Dim1'], df_tsne['Dim2'], c=df_tsne['category_encoded'], cmap='tab20', s=10, alpha=0.5)\n",
    "plt.title('t-SNE visualization of fasttext embeddings using category labels')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7hFJQ_CCnH5"
   },
   "source": [
    "We can see that the model is able to clearly distinguish some of the news categories and also identifies several well-defined clusters that could represent subcategories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a pickle file\n",
    "\n",
    "# embeddings_path = os.path.join(project_dir, 'news_train_fasttext_embeddings.pkl')\n",
    "\n",
    "# with open(embeddings_path, 'wb') as f:\n",
    "#     pickle.dump(news_train[['news_id', 'fasttext_embedding']], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from pickle to check if it works\n",
    "# with open(embeddings_path, 'rb') as f:\n",
    "#     ft_emb = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skTy8uUHv5Va"
   },
   "source": [
    "Generate the same embeddings for news dev dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dev_path = os.path.join(project_dir, 'MINDsmall_dev/news.tsv')\n",
    "\n",
    "news_dev = pd.read_csv(news_dev_path, sep='\\t', header=None, names=[\"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"title_entities\", \"abstract_entities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dev[\"abstract\"] = news_dev[\"abstract\"].fillna(\"\")\n",
    "news_dev['proc_subcategory'] = news_dev.apply(lambda row: preprocess_subcategory(row['category'], row['subcategory']), axis=1)\n",
    "\n",
    "news_dev[\"text\"] = news_dev[\"category\"] + \" \"\\\n",
    "+ news_dev[\"proc_subcategory\"] + \" \"\\\n",
    "+ news_dev[\"title\"] + \" \"\\\n",
    "+ news_dev[\"abstract\"]\n",
    "\n",
    "news_dev['clean_text'] = news_dev['text'].apply(preprocess_text)\n",
    "news_dev['fasttext_embedding'] = news_dev['clean_text'].apply(lambda x: get_mean_embedding(x, ft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_path = os.path.join(project_dir, 'news_dev_fasttext_embeddings.pkl')\n",
    "\n",
    "# with open(embeddings_path, 'wb') as f:\n",
    "#     pickle.dump(news_dev[['news_id', 'fasttext_embedding']], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from pickle to check if it works\n",
    "# with open(embeddings_path, 'rb') as f:\n",
    "#     ft_emb = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuWedmNDpfMh"
   },
   "source": [
    "### Approach 2: Use pre-trained SentenceBERT embeddings to get rich text representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT model and tokenizer\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute BERT embeddings for the input text\n",
    "def get_bert_embedding(text, tokenizer, model):\n",
    "    with torch.no_grad():\n",
    "        # Tokenize and encode the text with padding and truncation\n",
    "        inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "        # Move tensors to the appropriate device\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        # Pass inputs through the model to obtain outputs\n",
    "        outputs = model(**inputs)\n",
    "        # Extract and return the [CLS] token embedding as a NumPy array\n",
    "        return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for text in tqdm(news_train[\"text\"].fillna(\"\"), desc=\"Generating BERT embeddings\"):\n",
    "    embedding = get_bert_embedding(text, tokenizer, model)\n",
    "    embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train[\"bert_embedding\"] = embeddings\n",
    "\n",
    "# embeddings_path = os.path.join(project_dir, \"news_train_bert_embeddings.pkl\")\n",
    "\n",
    "# with open(embeddings_path, \"wb\") as f:\n",
    "#     pickle.dump(news_train[[\"news_id\", \"bert_embedding\"]], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(embeddings[0]) # Normalization might be needed later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9isZcfUYucFg"
   },
   "source": [
    "Generate the same embeddings for news dev dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dev_path = os.path.join(project_dir, 'MINDsmall_dev/news.tsv')\n",
    "\n",
    "news_dev = pd.read_csv(news_dev_path, sep='\\t', header=None, names=[\"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"title_entities\", \"abstract_entities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dev[\"abstract\"] = news_dev[\"abstract\"].fillna(\"\")\n",
    "news_dev['proc_subcategory'] = news_dev.apply(lambda row: preprocess_subcategory(row['category'], row['subcategory']), axis=1)\n",
    "\n",
    "news_dev[\"text\"] = news_dev[\"category\"] + \" \"\\\n",
    "+ news_dev[\"proc_subcategory\"] + \" \"\\\n",
    "+ news_dev[\"title\"] + \" \"\\\n",
    "+ news_dev[\"abstract\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for text in tqdm(news_dev[\"text\"].fillna(\"\"), desc=\"Generating BERT embeddings\"):\n",
    "    embedding = get_bert_embedding(text, tokenizer, model)\n",
    "    embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dev[\"bert_embedding\"] = embeddings\n",
    "\n",
    "# embeddings_path = os.path.join(project_dir, \"news_dev_bert_embeddings.pkl\")\n",
    "\n",
    "# with open(embeddings_path, \"wb\") as f:\n",
    "#     pickle.dump(news_dev[[\"news_id\", \"bert_embedding\"]], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dop57Kjl2Kzo"
   },
   "source": [
    "### Approach 3: Extract entity_embedding.vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wikidata_ids(entity_list):\n",
    "    \"\"\"\n",
    "    Extracts Wikidata IDs from a list of entity dictionaries.\n",
    "    If the entity_list is empty or malformed, returns an empty list.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        entities = json.loads(entity_list)\n",
    "        return [entity[\"WikidataId\"] for entity in entities if \"WikidataId\" in entity]\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train[\"title_wikidata_ids\"] = news_train[\"title_entities\"].fillna(\"[]\").apply(extract_wikidata_ids)\n",
    "news_train[\"abstract_wikidata_ids\"] = news_train[\"abstract_entities\"].fillna(\"[]\").apply(extract_wikidata_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train[['title_wikidata_ids', 'abstract_wikidata_ids']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train['title_wikidata_ids_len'] = news_train['title_wikidata_ids'].apply(lambda x: len(x))\n",
    "news_train['abstract_wikidata_ids_len'] = news_train['abstract_wikidata_ids'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(news_train['title_wikidata_ids_len'], bins=50, kde=True)\n",
    "plt.xlabel(\"Num Entities\")\n",
    "plt.title(\"Num of Title Entities\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(news_train['abstract_wikidata_ids_len'], bins=50, kde=True)\n",
    "plt.xlabel(\"Num Entities\")\n",
    "plt.title(\"Num of Abstract Entities\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_entity_embeddings(filepath):\n",
    "    entity_embeddings = {}\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            entity = parts[0]  # Entity ID (assuming first column is entity)\n",
    "            vector = np.array(parts[1:], dtype=np.float32)  # Embedding vector\n",
    "            entity_embeddings[entity] = vector\n",
    "    return entity_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_emb_path = os.path.join(project_dir, 'MINDsmall_train/entity_embedding.vec')\n",
    "relation_emb_path = os.path.join(project_dir, 'MINDsmall_train/relation_embedding.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_embeddings = load_entity_embeddings(entity_emb_path)\n",
    "relation_embeddings = load_entity_embeddings(relation_emb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_embedding(entity_ids, entity_embeddings, embedding_dim=100):\n",
    "    \"\"\"Retrieve entity embeddings and take mean if multiple entities exist.\"\"\"\n",
    "    valid_embeddings = [entity_embeddings[eid] for eid in entity_ids if eid in entity_embeddings]\n",
    "\n",
    "    if valid_embeddings:\n",
    "        return np.mean(valid_embeddings, axis=0)  # Average if multiple\n",
    "    else:\n",
    "        return np.zeros(embedding_dim)  # Zero vector if no entity found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_entity_vectors = []\n",
    "abstract_entity_vectors = []\n",
    "\n",
    "for _, row in tqdm(news_train.iterrows(), total=len(news_train), desc=\"Extracting entity embeddings\"):\n",
    "    # Get embeddings for title and abstract entities\n",
    "    title_vector = get_entity_embedding(row[\"title_wikidata_ids\"], entity_embeddings)\n",
    "    abstract_vector = get_entity_embedding(row[\"abstract_wikidata_ids\"], entity_embeddings)\n",
    "\n",
    "    # Store results\n",
    "    title_entity_vectors.append(title_vector)\n",
    "    abstract_entity_vectors.append(abstract_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_entity_vectors = np.array(title_entity_vectors)\n",
    "abstract_entity_vectors = np.array(abstract_entity_vectors)\n",
    "\n",
    "# Create a final 200-dimensional entity embedding per article\n",
    "news_train[\"entity_embedding\"] = list(np.hstack((title_entity_vectors, abstract_entity_vectors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dev_path = os.path.join(project_dir, 'MINDsmall_dev/news.tsv')\n",
    "news_dev = pd.read_csv(news_dev_path, sep='\\t', header=None, names=[\"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"title_entities\", \"abstract_entities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dev[\"title_wikidata_ids\"] = news_dev[\"title_entities\"].fillna(\"[]\").apply(extract_wikidata_ids)\n",
    "news_dev[\"abstract_wikidata_ids\"] = news_dev[\"abstract_entities\"].fillna(\"[]\").apply(extract_wikidata_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_emb_path = os.path.join(project_dir, 'MINDsmall_dev/entity_embedding.vec')\n",
    "entity_embeddings = load_entity_embeddings(entity_emb_path)\n",
    "\n",
    "title_entity_vectors = []\n",
    "abstract_entity_vectors = []\n",
    "\n",
    "for _, row in tqdm(news_dev.iterrows(), total=len(news_dev), desc=\"Extracting entity embeddings\"):\n",
    "    # Get embeddings for title and abstract entities\n",
    "    title_vector = get_entity_embedding(row[\"title_wikidata_ids\"], entity_embeddings)\n",
    "    abstract_vector = get_entity_embedding(row[\"abstract_wikidata_ids\"], entity_embeddings)\n",
    "\n",
    "    # Store results\n",
    "    title_entity_vectors.append(title_vector)\n",
    "    abstract_entity_vectors.append(abstract_vector)\n",
    "\n",
    "title_entity_vectors = np.array(title_entity_vectors)\n",
    "abstract_entity_vectors = np.array(abstract_entity_vectors)\n",
    "\n",
    "# Create a final 200-dimensional entity embedding per article\n",
    "news_dev[\"entity_embedding\"] = list(np.hstack((title_entity_vectors, abstract_entity_vectors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_path = os.path.join(project_dir, \"news_train_entity_embeddings.pkl\")\n",
    "\n",
    "# with open(embeddings_path, \"wb\") as f:\n",
    "#     pickle.dump(news_train[[\"news_id\", \"entity_embedding\"]], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_path = os.path.join(project_dir, \"news_dev_entity_embeddings.pkl\")\n",
    "\n",
    "# with open(embeddings_path, \"wb\") as f:\n",
    "#     pickle.dump(news_dev[[\"news_id\", \"entity_embedding\"]], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
